#!/usr/bin/env perl
use strict;
use feature 'unicode_strings';
use warnings FATAL => "utf8";

# Non-core dependencies
#
use JSON::Tiny qw(decode_json encode_json);
use PerlIO::gzip;

# Core dependencies
#
use File::Spec;

=head1 NAME

canto_compile.pl - Compile data files for Cantotype.

=head1 SYNOPSIS

  canto_compile.pl
    -hkscs HKSCS.json
    -unihan Unihan/folder
    -cedict cedict.txt
    -outdir out/dir
    -csplit 2
    -wsplit 5

=head1 DESCRIPTION

This script reads data sources from Hong Kong Supplemental Character Set
(HKSCS), Unihan, and CC-CEDICT and uses this data to compile the data
files for Cantotype.

The paths to the HKSCS supplement JSON file and the decompressed
CC-CEDICT data file are given using the C<-hkscs> and C<-cedict>
options, respectively.  The Unihan database is split across multiple
files.  The folder containing these Unihan files is given using the
C<-unihan> option, and the files within that folder must have the same
names that they had in the source C<unihan.zip> file.  See the section
on "Data sources" below for further information where to obtain these
data sources.

This script generates multiple data files, along with a data index file.
All these files will be written into the output directory that is given
with the C<-outdir> parameter.

The C<-csplit> and C<-wsplit> options are optional, and default to
values of one if not given.  The C<-csplit> option indicates how many
file parts the character data should be split into, while the C<-wsplit>
option indicates how many file parts the word data should be split into.
This can be used to split the data files into reasonable-sized chunks to
make the Cantotype loading process smoother.  The minimum split value is
one and the maximum split value is 999.

In the output directory, all files are JSON data files that are
compressed with GZip.  Cantotype expects to download these files in the
compressed format, so they should not be decompressed.  The data index
file will have the name C<cantotype_data_index.gz>.  All character data
files will have the name format C<cantotype_data_cNNN.gz> where C<NNN>
is a three-digit, zero-padded part number that begins with part one.
All word data files will have the name format C<cantotype_data_wNNN.gz>
where C<NNN> is a three-digit, zero-padded part number that begins with
part one.

The character data files and word data files must keep their names as-is
because their filenames are referenced in the data index file.  However,
the data index file can be renamed.

=head2 Data file format

All data files are stored in the JSON format, and then compressed with
Gzip.

The character data table and word data table are both JSON arrays where
each element of the array is a data record.  Records are not stored in
any particular order.

Since both tables may potentially be very large, they may be split
across multiple files.  Each individual file must be a fully valid JSON
file.  Each file part is a JSON array that stores only a subset of the
records.  All of the file parts taken together define the total
collection of records.

There is also a data index file.  This file simply stores the filenames
of all the component data files.  It is also a compressed JSON file.
The data index JSON file stores a JSON object that has two properties:
C<charlist> and C<wordlist>.  Both of these properties have values that
are arrays of strings.  The C<charlist> property gives the filenames of
all the data files that define the character table, while the
C<wordlist> property gives the filenames of all the data files that
define the word table.

The data index file only determines the filenames of the component data
files, not their complete URLs.  The URL of the directory that contains
the component data files is determined by the configuration script
generated by C<canto_config.pl>.  This allows the same set of generated
data files to be used regardless of their specific location.

=head3 Character table format

For the character table array, each record is a JavaScript object whose
properties define the properties of a specific Chinese character.  The
following properties are always defined:

=over 4

=item C<cpv>

The numeric Unicode codepoint of the character.  Supplemental characters
are represented directly by their numeric codepoint rather than by a
surrogate pair.  Each record in the C<canto_chars> array has a unique
value for this field, but there is no guarantee of the ordering of
records in the table.  This field is an integer value.

=item C<crd>

The Cantonese reading(s) of the character.  The value is an array of one
or more strings, each of which contains a lowercase Jyutping reading.
No particular ordering is guaranteed within the readings.

=back

The following properties are optional, only defined if available:

=over 4

=item C<dfn>

A string providing a definition gloss of the character's meaning, in
English.  This is taken from the Unihan database, so it is not meant to
be an actual Chinese word definition, but rather just a gloss for that
specific character.

=back

=head3 Word table format

For the word table array, each record is itself an array.  The elements
of the array are as follows:

=over 4

=item Element 0

String value holding the traditional character(s) for the word.  For
western names, double-width middle dots may be present.  For proverbs,
double-width commas may be present.

=item Element 1

String value holding the simplified character(s) for the word.  For
western names, double-width middle dots may be present.  For proverbs,
double-width commas may be present.

=item Element 2

Array of strings indicating the Mandarin Pinyin reading of the word.  If
the array has multiple elements, this means a multi-syllable
pronunication.  It does not mean each element is an alternative.  Pinyin
syllables may have their initial letters capitalized for proper names.
Diacritic marks are not used.  Instead, tone is represented by an
integer value 1-5 suffixed to the syllable, with 5 representing neutral
tone.  Also, the U-umlaut character is represented by the letter U
followed by a colon.  Finally, if a double-width middle dot or a
double-width comma was part of the prior record elements, those
punctuation marks will appear as their own element within this reading
array, with a regular middle dot and regular comma used instead of the
double-width varieties.

=item Element 3

Array of strings, each containing a separate English definition of the
Chinese word.  Chinese characters might be included within these
definitions, so it is not safe to assume they are ASCII.

=back

=head2 Character table range

The character table contains a subset of the Chinese characters in
Unicode.  Specifically, the table only includes characters that have a
Big5 mapping defined in the Unihan database B<or> appear in the HKSCS
supplement of Cantonese-specific characters.  Furthermore, only
characters that have at least one Cantonese reading will be included.

=head2 Jyutping format

The C<VALID_INITIALS> and C<VALID_FINALS> variables give the recognized
valid initials and finals in Jyutping syllables.  The tone numbers may
be 1-6.  Also, finals "m" and "ng" may stand by themselves without a
vowel in the final.

=head2 Data sources

The HKSCS file is a JSON file that has a JSON array at the top level.
Each element in this JSON array is a JSON object.  These object elements
must at least have a C<codepoint> property and a C<cantonese> property.
The C<codepoint> property has a string value that contains the Unicode
codepoint as base-16 digits.  The C<cantonese> property is also a
string.  If it is empty, the record is ignored.  Otherwise, it must
consist of one or more Jyutping romanizations, separated by commas.

The HKSCS file is B<not> a complete listing of Cantonese characters.
Instead, it only contains special Cantonese characters that are beyond
the baseline Big5 standard.  You can get a file C<HKSCS2016.json> that
is entitled I<Hong Kong Supplementary Character Set related information
(JSON format)> which has the necessary format from the Office of the
Government Chief Information Officer of Hong Kong, Common Chinese
Language Interface, Download Area, at:

C<https://www.ogcio.gov.hk/en/our_work/business/tech_promotion/ccli/download_area/>

The Unihan files are available from the Unicode consortium in an archive
called C<unihan.zip>.  The archive is available at:

C<https://www.unicode.org/Public/UCD/latest/ucd/Unihan.zip>

This script requires the path to a directory that contains all of the
text files extracted from C<unihan.zip>.

The HKSCS and Unihan data files have some typos and obscure characters
in them that this script automatically corrects.  The correction
procedure is smart enough that if the datasets are fixed in the futures,
the corrections will no longer be applied in the future if no longer
necessary.  See C<correct_cmap> function for details of the corrections
that are applied.

Finally, the script requires the CC-CEDICT Chinese dictionary data file.
This file is available for download from C<www.mdbg.net>.  You must
decompress it before passing it to this script.

=cut

# =========
# Constants
# =========

# All of the valid Jyutping initials, in lowercase, with each initial
# surrounded by colons
#
my $VALID_INITIALS = ":b:p:m:f:d:t:n:l:g:k:ng:h:gw:kw:w:z:c:s:j:";

# All of the valid Jyutping finals, in lowercase, with each final
# surrounded by colons
#
# NOTE: "et" final added in here because it is used in multiple entries
#
my $VALID_FINALS =
  ":aa:aai:aau:aam:aan:aang:aap:aat:aak" .
  ":a:ai:au:am:an:ang:ap:at:ak" .
  ":e:ei:eu:em:eng:ep:et:ek" .
  ":i:iu:im:in:ing:ip:it:ik" .
  ":o:oi:ou:on:ong:ot:ok" .
  ":u:ui:un:ung:ut:uk" .
  ":eoi:eon:eot" .
  ":oe:oeng:oet:oek" .
  ":yu:yun:yut" .
  ":m:ng:";

# ===============
# Local functions
# ===============

# Given a reference to the %cmap hash, apply corrections for typos and
# obscure characters in the input datasets.
#
# IMPORTANT: do this *before* upgrading the cmap with upgrade_cmap().
#
# The following obscure codepoints with non-conformant Jyutping are
# DROPPED from the %cmap if present:
#
#   U+297C4 ("qi1")
#   U+2BAC8 ("bop6")
#
# The following codepoints, if present, have the following corrections
# made to their readings, if the error version is present:
#
#   U+5414  yaa1   -> jaa1
#   U+667B  om2    -> am2
#   U+27C3C zeong6 -> zoeng6
#   U+2BAF2 zoen6  -> zeon6
#   U+2BB1B zoen6  -> zeon6
#   U+2F817 yung2  -> jung2
#
# Parameters:
#
#   1 : hash ref - the %cmap to apply corrections to
#
sub correct_cmap {
  # Check number of parameters
  ($#_ == 0) or die "Wrong number of parameters, stopped";
  
  # Get argument and check type
  my $cm = shift;
  (ref($cm) eq "HASH") or die "Wrong argument type, stopped";
  
  # Drop the obscure codepoints if present
  for my $cpv (0x297c4, 0x2bac8) {
    if (exists $cm->{"$cpv"}) {
      delete $cm->{"$cpv"};
    }
  }
  
  # Apply corrections
  for my $ca ([ 0x5414, "yaa1"  , "jaa1"  ],
              [ 0x667b, "om2"   , "am2"   ],
              [0x27c3c, "zeong6", "zoeng6"],
              [0x2baf2, "zoen6" , "zeon6" ],
              [0x2bb1b, "zoen6" , "zeon6" ],
              [0x2f817, "yung2" , "jung2" ]) {
    
    # Get parameters for this correction
    my $cpv = $ca->[0];
    my $cer = $ca->[1];
    my $crp = $ca->[2];

    # Skip if codepoint not present
    if (not exists $cm->{"$cpv"}) {
      next;
    }

    # Get reference to readings array
    my $ra = $cm->{"$cpv"};
    
    # Starting from last element of readings array and going to first,
    # drop any readings that are erroneous and set the found_err flag if
    # at least one reading dropped
    my $found_err = 0;
    for(my $i = (scalar @$ra - 1); $i >= 0; $i--) {
      if ($ra->[$i] eq $cer) {
        $found_err = 1;
        splice @$ra, $i, 1;
      }
    }

    # Skip rest of processing if no error reading found
    if (not $found_err) {
      next;
    }
    
    # Check whether the correction is already in the readings array
    my $already = 0;
    for my $r (@$ra) {
      if ($r eq $crp) {
        $already = 1;
        last;
      }
    }

    # Add correction if not already in readings array
    if (not $already) {
      push @$ra, ($crp);
    }
  }
}

# Check whether a given string is a valid Jyutping syllable.
#
# Parameters:
#
#   1 : string - the string to check
#
# Return:
#
#   1 if string is valid Jyutping syllable, 0 if not
#
sub check_reading {
  # Check number of parameters
  ($#_ == 0) or die "Wrong number of parameters, stopped";
  
  # Get argument as string
  my $str = shift;
  $str = "$str";
  
  # Check for tone suffix and get the main syllable
  ($str =~ /^([a-z]+)[1-6]$/u) or return 0;
  $str = $1;
  
  # Handle syllabic m and ng as special case
  if ($str =~ /^([^aeiouy]*)(?:m|ng)$/u) {
    # Get the initial
    $str = $1;
    
    # If initial is not empty, check that initial is valid
    ((length($str) < 1) or ($VALID_INITIALS =~ /:$str:/u)) or return 0;
    
    # If we got here, syllable is valid as special case with syllabic
    # m or ng
    return 1;
  }
  
  # Split into initial (which may be empty) and final (required)
  ($str =~ /^([^aeiouy]*)([aeiouy].*)$/u) or return 0;
  my $i = $1;
  my $f = $2;
  
  # If initial is not empty, check that initial is valid
  ((length($i) < 1) or ($VALID_INITIALS =~ /:$i:/u)) or return 0;
  
  # Check that final is valid
  ($VALID_FINALS =~ /:$f:/u) or return 0;
  
  # If we got here, syllable is valid Jyutping
  return 1;
}

# Given a path to a directory and a filename within that directory,
# return the full path to that file within the directory.
#
# Parameters:
#
#   1 : string - the path to the directory
#
#   2 : string - the filename
#
# Return:
#
#   string - the path to the file
#
sub subfile {
  # Check parameter count
  ($#_ == 1) or die "Wrong number of parameters, stopped";
  
  # Get parameters as strings
  my $path_dir  = shift;
  my $path_name = shift;
  
  $path_dir  = "$path_dir";
  $path_name = "$path_name";
  
  # Split the directory
  (my $dvol, my $ddir, undef) = File::Spec->splitpath($path_dir, 1);
  
  # Return the full path
  return File::Spec->catpath($dvol, $ddir, $path_name);
}

# Given a reference to the %cmap hash, initialize it with the core Big5
# codepoints from the Unihan database.
#
# The %cmap hash should be empty when this function is called.
#
# Only core Big5 codepoints defined in Unihan will be added by this
# function.  This does NOT include Cantonese specific extensions with
# HKSCS.
#
# The keys within the %cmap will be set to unsigned decimal integer
# strings for the codepoint value.  The values within the %cmap will all
# be set to empty arrays.
#
# Parameters:
#
#   1 : hash ref - reference to the %cmap
#
#   2 : string - path to the "other mappings" Unihan data file
#
sub grab_big5 {
  # Check parameter count
  ($#_ == 1) or die "Wrong number of parameters, stopped";
  
  # Get parameters and check types
  my $cm        = shift;
  my $data_path = shift;
  
  (ref($cm) eq 'HASH') or die "Wrong parameter type, stopped";
  $data_path = "$data_path";
  
  # Open the other mappings file
  open(my $fhm, "< :utf8", $data_path) or
    die "Failed to open '$data_path', stopped";
  
  # Process mappings file line by line and add all Big5 Unicode
  # codepoints to the hash, with empty array reference values for now
  while (<$fhm>) {
    # Skip line if blank
    if (/^[ \t\r\n]*$/u) {
      next;
    }
    
    # Skip line if first character is # indicating comment
    if (/^[ \t]*#/u) {
      next;
    }
    
    # If this is a Big5 record, add to the hash
    if (/^[ \t]*U\+([0-9a-fA-F]{4,6})\tkBigFive\t/u) {
      my $cpv = hex($1);
      $cm->{"$cpv"} = [];
    }
  }
  
  # Close the mappings file
  close($fhm);
}

# Given a reference to a %cmap hash that has been initialized, add
# Cantonese readings from the Unihan data.
#
# The %cmap should have been run through grab_big5() first.  That
# function adds codepoint entries for all core Big5 codepoints in the
# Unihan data, and maps each of them to empty array references.
#
# This function will run through all the Cantonese readings in the
# Unihan database and add readings to the codepoints that are already in
# %cmap.  No new codepoints will be added to %cmap by this function.
#
# Parameters:
#
#   1 : hash ref - reference to the %cmap
#
#   2 : string - path to the "readings" Unihan data file
#
sub apply_canto {
  # Check parameter count
  ($#_ == 1) or die "Wrong number of parameters, stopped";
  
  # Get parameters and check types
  my $cm        = shift;
  my $data_path = shift;
  
  (ref($cm) eq 'HASH') or die "Wrong parameter type, stopped";
  $data_path = "$data_path";
  
  # Open the readings file
  open(my $fhr, "< :utf8", $data_path) or
    die "Failed to open '$data_path', stopped";
  
  # Process readings file line by line, and for any Cantonese reading
  # where the codepoint is already in the mappings file, push all
  # readings onto the array value, after making sure the array doesn't
  # already contain that reading
  while (<$fhr>) {
    # Skip line if blank
    if (/^[ \t\r\n]*$/u) {
      next;
    }
    
    # Skip line if first character is # indicating comment
    if (/^[ \t]*#/u) {
      next;
    }
    
    # If this is a Cantonese reading record, process it
    if (/^[ \t]*U\+([0-9a-fA-F]{4,6})\tkCantonese\t(.+)[\r\n]*$/u) {
      my $cpv = hex($1);
      my $rstr = $2;
      
      # Check that exactly one syllable defined
      ($rstr =~ /^[A-Za-z]+[1-6]$/u) or
        die "Invalid kCantonese value: $rstr stopped";
      
      # Check if reading already present
      my $already = 0;
      for my $r (@{$cm->{"$cpv"}}) {
        if ($r eq $rstr) {
          $already = 1;
          last;
        }
      }
      
      # If not already defined, add the reading
      if (not $already) {
        push @{$cm->{"$cpv"}}, ($rstr);
      }
    }
  }
  
  # Close the readings file
  close($fhr);  
}

# Given a reference to a %cmap hash, add extra Cantonese readings and
# codepoints from the HKSCS supplement.
#
# This function will run through all the codepoints in the HKSCS
# supplement that have at least one Cantonese reading, add the
# codepoints to the %cmap if not already defined, and add the HKSCS
# readings to the codepoint if not already present.
#
# Parameters:
#
#   1 : hash ref - reference to the %cmap
#
#   2 : string - path to the HKSCS data file
#
sub apply_hkscs {
  # Check parameter count
  ($#_ == 1) or die "Wrong number of parameters, stopped";
  
  # Get parameters and check types
  my $cm        = shift;
  my $data_path = shift;
  
  (ref($cm) eq 'HASH') or die "Wrong parameter type, stopped";
  $data_path = "$data_path";
  
  # Open the HKSCS file in raw mode
  open(my $fhh, "< :raw", $data_path) or
    die "Failed to open '$data_path', stopped";
  
  # Slurp the whole HKSCS file into memory
  my $hkscs;
  {
    local $/;
    $hkscs = <$fhh>;
  }
  
  # Close HKSCS file
  close($fhh);
  
  # If file starts with UTF-8 BOM, remove it
  if (length $hkscs > 3) {
    if ((ord(substr($hkscs, 0, 1)) == 0xef) and
        (ord(substr($hkscs, 1, 1)) == 0xbb) and
        (ord(substr($hkscs, 2, 1)) == 0xbf)) {
      $hkscs = substr($hkscs, 3);
    }
  }
  
  # Decode JSON
  $hkscs = decode_json($hkscs);
  
  # Make sure top-level JSON is array
  (ref($hkscs) eq "ARRAY") or
    die "HKSCS must be JSON array, stopped";
  
  # Go through all JSON records, and for each that has a Cantonese
  # reading, add it to the hash, making sure it's not already in there
  for my $h (@$hkscs) {
    
    # Make sure element is hash reference
    (ref($h) eq "HASH") or
      die "HKSCS elements must be JSON objects, stopped";
    
    # Make sure required codepoint and cantonese properties are there
    (exists $h->{'codepoint'}) or
      die "HKSCS elements must all have codepoint properties, stopped";
    (exists $h->{'cantonese'}) or
      die "HKSCS elements must all have cantonese properties, stopped";
    
    # Get the properties
    my $cpv  = $h->{'codepoint'};
    my $cstr = $h->{'cantonese'};
    
    # Skip if cantonese property is empty
    if ($cstr =~ /^[ \t]*$/u) {
      next;
    }
    
    # Parse the codepoint
    $cpv = hex($cpv);
    
    # Replace periods with commas -- this will fix the "nuk6." typo in
    # HKSCS2016
    $cstr =~ s/\./,/ug;
    
    # Replace all commas with comma followed by space so that there is a
    # space everywhere after commas -- this will fix the "taan3,wan6"
    # typo that is missing a space in HKSCS2016
    $cstr =~ s/,/, /ug;
    
    # Drop all commas -- we can parse the syllables with spaces, and
    # there is a typo in HKSCS2016 at "jing1 mau5" with a missing comma
    # that will be fixed if we go by whitespace instead
    $cstr =~ s/,//ug;
    
    # Trim leading and trailing whitespace
    $cstr =~ s/^[ \t]+//gu;
    $cstr =~ s/[ \t]+$//gu;
    
    # Split by whitespace
    my @car = split " ", $cstr;
    
    # Process each reading
    for my $r (@car) {
      # If codepoint doesn't exist yet, add it with empty array
      if (not exists $cm->{"$cpv"}) {
        $cm->{"$cpv"} = [];
      }
      
      # Check whether reading already present in array
      my $already = 0;
      for my $s (@{$cm->{"$cpv"}}) {
        if ($s eq $r) {
          $already = 1;
          last;
        }
      }
      
      # If not already defined, add the reading
      if (not $already) {
        push @{$cm->{"$cpv"}}, ($r);
      }
    }
  }
}

# Upgrade a %cmap reference from just storing Cantonese readings for
# each codepoint to storing a full descriptive object for each
# codepoint.
#
# Each value in the %cmap must be an array reference when this function
# is called.
#
# The array reference values will be converted into hash reference
# values where the original array reference is stored in a "crd"
# property and the numeric codepoint value is stored in a "cpv"
# property.
#
# This function will also check that all the Jyutping readings are
# valid, using check_reading().
#
# Parameters:
#
#   1 : hash ref - reference to the %cmap
#
sub upgrade_cmap {
  # Check parameter count
  ($#_ == 0) or die "Wrong number of parameters, stopped";
  
  # Get parameter and check type
  my $cm = shift;
  (ref($cm) eq 'HASH') or die "Wrong parameter type, stopped";
  
  # Go through all the keys and upgrade the values to hash references
  for my $k (keys %$cm) {
    # Get current value
    my $old_val = $cm->{$k};
    
    # Make sure old value is an array reference
    (ref($old_val) eq 'ARRAY') or die "Wrong cmap state, stopped";
    
    # Check all the readings in the array
    for my $r (@$old_val) {
      (check_reading($r)) or die "Invalid Jyutping '$r', stopped";
    }
    
    # Define a new hash for the new value
    my %h;
    
    # Add the properties
    $h{'cpv'} = int($k);
    $h{'crd'} = $old_val;
    
    # Upgrade this key in the cmap
    $cm->{$k} = \%h;
  }
}

# Given a %cmap that has been upgraded with upgrade_cmap(), add "dfn"
# properties for any kDefinition entries found in the Unihan database.
#
# Parameters:
#
#   1 : hash ref - reference to the %cmap
#
#   2 : string - path to the Unihan "readings" data file
#
sub add_defns {
  # Check parameter count
  ($#_ == 1) or die "Wrong number of parameters, stopped";
  
  # Get parameters and check types
  my $cm        = shift;
  my $data_path = shift;
  
  (ref($cm) eq 'HASH') or die "Wrong parameter type, stopped";
  $data_path = "$data_path";
  
  # Open the readings file so we can add definitions
  open(my $fhd, "< :utf8", $data_path) or
    die "Failed to open '$data_path', stopped";
  
  # Process readings file line by line, and add any definitions to
  # codepoints already in %cmap
  while (<$fhd>) {
    # Skip line if blank
    if (/^[ \t\r\n]*$/u) {
      next;
    }
    
    # Skip line if first character is # indicating comment
    if (/^[ \t]*#/u) {
      next;
    }
    
    # If this is a definition record, process it
    if (/^[ \t]*U\+([0-9a-fA-F]{4,6})\tkDefinition\t(.+)[\r\n]*$/u) {
      my $cpv = hex($1);
      my $dstr = $2;
      
      # Skip this definition record if it is not in the %cmap
      if (not exists $cm->{$cpv}) {
        next;
      }
      
      # Store the definition
      $cm->{$cpv}->{'dfn'} = $dstr;
    }
  }
  
  # Close the readings file
  close($fhd);
}

# Given an array reference, add dictionary entries from the CC-CEDICT
# dictionary.
#
# Each dictionary entry is parsed from the CC-CEDICT data file and then
# pushed onto the end of the given array reference.
#
# Parameters:
#
#   1 : array ref - reference to the dictionary array
#
#   2 : string - path to the CC-CEDICT data file
#
sub import_dictionary {
  # Check parameter count
  ($#_ == 1) or die "Wrong number of parameters, stopped";
  
  # Get parameters and check types
  my $dm        = shift;
  my $data_path = shift;
  
  (ref($dm) eq 'ARRAY') or die "Wrong parameter type, stopped";
  $data_path = "$data_path";
  
  # Open the dictionary file so we can import records
  open(my $fhd, "< :utf8", $data_path) or
    die "Failed to open '$data_path', stopped";
  
  # Process dictionary file line by line, and add definition records to
  # the array reference
  while (<$fhd>) {
    # Skip line if blank
    if (/^[ \t\r\n]*$/u) {
      next;
    }
    
    # Skip line if first character is # indicating comment
    if (/^[ \t]*#/u) {
      next;
    }
    
    # Parse the dictionary record
    (/^([^ ]+) ([^ ]+) \[([^\]]*)\] \/([^\r\n]*)\/[ \t\r\n]*$/u) or
      die "Invalid CC-CEDICT record '$_', stopped";
    
    my $rf_trad = $1;
    my $rf_simp = $2;
    my $rf_piny = $3;
    my $rf_dfns = $4;

    # Traditional and simplified reading fields are as-is strings
    $rf_trad = "$rf_trad";
    $rf_simp = "$rf_simp";
    
    # Begin by trimming leading and trailing whitespace from pinyin
    # string
    $rf_piny = "$rf_piny";
    $rf_piny =~ s/^[ \t]+//gu;
    $rf_piny =~ s/[ \t]+$//gu;
    
    # Split pinyin reading into entities according to whitespace
    # separators
    my @pya = split " ", $rf_piny;
    
    # For definitions string, split by "/" marks (the opening and
    # closing slash marks are not included in the definitions string)
    my @dfa = split /\//, $rf_dfns;
    
    # Trim each definition of leading and trailing whitespace
    for(my $i = 0; $i <= $#dfa; $i++) {
      $dfa[$i] =~ s/^[ \t]+//gu;
      $dfa[$i] =~ s/[ \t]+$//gu;
    }

    # Push the dictionary record to the end of the dictionary array
    push @$dm, ([$rf_trad, $rf_simp, \@pya, \@dfa]);
  }
  
  # Close the dictionary file
  close($fhd);
}

# ==================
# Program entrypoint
# ==================

# Define a hash that will store parameters we receive on the command
# line
#
my %script_param;

# Parse the command-line parameters and add to %script_param
#
for(my $i = 0; $i <= $#ARGV; $i++) {
  # Get parameter name
  my $p = $ARGV[$i];
  
  # Handle parameter
  if ($p eq "-hkscs") { # ----------------------------------------------
    # Make sure at least one extra parameter
    ($i < $#ARGV) or die "-hkscs requires parameter, stopped";
    
    # Advance to next parameter and get it
    $i++;
    $p = $ARGV[$i];
    
    # Check that parameter not already defined
    (not exists $script_param{'hkscs'}) or 
      die "-hkscs defined twice, stopped";
    
    # Check that file exists
    (-f $p) or die "Can't find file '$p', stopped";
    
    # Store parameter
    $script_param{'hkscs'} = "$p";
    
  } elsif ($p eq "-unihan") { # ----------------------------------------
    # Make sure at least one extra parameter
    ($i < $#ARGV) or die "-unihan requires parameter, stopped";
    
    # Advance to next parameter and get it
    $i++;
    $p = $ARGV[$i];
    
    # Check that parameter not already defined
    (not exists $script_param{'unihan'}) or 
      die "-unihan defined twice, stopped";
    
    # Check that directory exists
    (-d $p) or die "Can't find directory '$p', stopped";
    
    # Store parameter
    $script_param{'unihan'} = "$p";
    
  } elsif ($p eq "-cedict") { # ----------------------------------------
    # Make sure at least one extra parameter
    ($i < $#ARGV) or die "-cedict requires parameter, stopped";
    
    # Advance to next parameter and get it
    $i++;
    $p = $ARGV[$i];
    
    # Check that parameter not already defined
    (not exists $script_param{'cedict'}) or 
      die "-cedict defined twice, stopped";
    
    # Check that file exists
    (-f $p) or die "Can't find file '$p', stopped";
    
    # Store parameter
    $script_param{'cedict'} = "$p";
    
  } elsif ($p eq "-outdir") { # ----------------------------------------
    # Make sure at least one extra parameter
    ($i < $#ARGV) or die "-outdir requires parameter, stopped";
    
    # Advance to next parameter and get it
    $i++;
    $p = $ARGV[$i];
    
    # Check that parameter not already defined
    (not exists $script_param{'outdir'}) or 
      die "-outdir defined twice, stopped";
    
    # Check that directory exists
    (-d $p) or die "Can't find directory '$p', stopped";
    
    # Store parameter
    $script_param{'outdir'} = "$p";
    
  } elsif ($p eq "-csplit") { # ----------------------------------------
    # Make sure at least one extra parameter
    ($i < $#ARGV) or die "-csplit requires parameter, stopped";
    
    # Advance to next parameter and get it
    $i++;
    $p = $ARGV[$i];
    
    # Check that parameter not already defined
    (not exists $script_param{'csplit'}) or 
      die "-csplit defined twice, stopped";
    
    # Check format
    ($p =~ /^[1-9][0-9]*$/) or die "Invalid -csplit value, stopped";
    
    # Convert to integer and check range
    $p = int($p);
    (($p > 0) and ($p < 1000)) or
      die "-csplit value out of range, stopped";
    
    # Store parameter (as integer)
    $script_param{'csplit'} = $p;
    
  } elsif ($p eq "-wsplit") { # ----------------------------------------
    # Make sure at least one extra parameter
    ($i < $#ARGV) or die "-wsplit requires parameter, stopped";
    
    # Advance to next parameter and get it
    $i++;
    $p = $ARGV[$i];
    
    # Check that parameter not already defined
    (not exists $script_param{'wsplit'}) or 
      die "-wsplit defined twice, stopped";
    
    # Check format
    ($p =~ /^[1-9][0-9]*$/) or die "Invalid -wsplit value, stopped";
    
    # Convert to integer and check range
    $p = int($p);
    (($p > 0) and ($p < 1000)) or
      die "-wsplit value out of range, stopped";
    
    # Store parameter (as integer)
    $script_param{'wsplit'} = $p;
    
  } else { # -----------------------------------------------------------
    die "Unrecognized parameter '$p', stopped";
  }
}

# Define default values of csplit and wsplit if they were not explicitly
# passed to the script
#
if (not exists $script_param{'csplit'}) {
  $script_param{'csplit'} = 1;
}
if (not exists $script_param{'wsplit'}) {
  $script_param{'wsplit'} = 1;
}

# Make sure all necessary parameters are now defined
#
for my $p ('hkscs', 'unihan', 'cedict', 'outdir', 'csplit', 'wsplit') {
  (exists $script_param{$p}) or
    die "Missing parameter -$p, stopped";
}

# Derive Unihan data file paths and add them to the script parameters
#
$script_param{'unihan_irg'}   = subfile(
                                  $script_param{'unihan'},
                                  "Unihan_IRGSources.txt");
$script_param{'unihan_other'} = subfile(
                                  $script_param{'unihan'},
                                  "Unihan_OtherMappings.txt");
$script_param{'unihan_radst'} = subfile(
                                  $script_param{'unihan'},
                                  "Unihan_RadicalStrokeCounts.txt");
$script_param{'unihan_read'}  = subfile(
                                  $script_param{'unihan'},
                                  "Unihan_Readings.txt");

# Check for existence of Unihan data files
#
(-f $script_param{'unihan_irg'}) or
  die "Can't find file '$script_param{'unihan_irg'}', stopped";
(-f $script_param{'unihan_other'}) or
  die "Can't find file '$script_param{'unihan_other'}', stopped";
(-f $script_param{'unihan_radst'}) or
  die "Can't find file '$script_param{'unihan_radst'}', stopped";
(-f $script_param{'unihan_read'}) or
  die "Can't file file '$script_param{'unihan_read'}', stopped";

# Start with an empty hash, which will map decimal integer codepoints to
# array references containing Jyutping romanizations
#
my %cmap;

# First, grab all the core Big5 codepoints and map them to empty array
# references for now
#
grab_big5(\%cmap, $script_param{'unihan_other'});

# Second, add all Cantonese readings to those codepoints, using the
# Unihan database
#
apply_canto(\%cmap, $script_param{'unihan_read'});

# Third, add any additional Cantonese codepoints and readings from the
# HKSCS supplement file
#
apply_hkscs(\%cmap, $script_param{'hkscs'});

# Apply corrections to %cmap and upgrade to have objects for each of the
# codepoints
#
correct_cmap(\%cmap);
upgrade_cmap(\%cmap);

# Add any kDefinition fields that we find in Unihan
#
add_defns(\%cmap, $script_param{'unihan_read'});

# Convert the %cmap into an array @car
#
my @cmap_keys = keys %cmap;
@cmap_keys = sort { int($a) <=> int($b) } @cmap_keys;

my @car;
for my $k (@cmap_keys) {
  push @car, ($cmap{$k});
}

# Import the CC-CEDICT dictionary
#
my @dar;
import_dictionary(\@dar, $script_param{'cedict'});

# Make sure neither table is empty
#
($#car > 0) or die "Character table is empty, stopped";
($#dar > 0) or die "Word table is empty, stopped";

# If the character split value is larger than the number of character
# records, set it to the number of character records
#
if ($script_param{'csplit'} > $#car + 1) {
  $script_param{'csplit'} = $#car + 1;
}

# If the word split value is larger than the number of word records, set
# it to the number of word records
#
if ($script_param{'wsplit'} > $#dar + 1) {
  $script_param{'wsplit'} = $#dar + 1;
}

# Get the volume and directory components to the output directory
#
(my $outdir_vol, my $outdir_path, undef) =
  File::Spec->splitpath($script_param{'outdir'}, 1);

# Generate the paths and filenames to all the character data files and
# all the word data files; each array element is a subarray where the
# first element of the subarray is the path to the file to generate on
# the local file system and the second element of the subarray is the
# filename that will be recorded in the data file index
# 
my @car_files;
my @dar_files;

for(my $i = 1; $i <= $script_param{'csplit'}; $i++) {
  my $fname = sprintf("cantotype_data_c%03d.gz", $i);
  
  push @car_files, ([
    File::Spec->catpath($outdir_vol, $outdir_path, $fname),
    $fname
  ]);
}

for(my $i = 1; $i <= $script_param{'wsplit'}; $i++) {
  my $fname = sprintf("cantotype_data_w%03d.gz", $i);
  
  push @dar_files, ([
    File::Spec->catpath($outdir_vol, $outdir_path, $fname),
    $fname
  ]);
}

# Generate the data file index object
#
my %dfix;

$dfix{'charlist'} = [];
$dfix{'wordlist'} = [];

for(my $i = 0; $i < $script_param{'csplit'}; $i++) {
  push @{$dfix{'charlist'}}, ($car_files[$i][1]);
}
for(my $i = 0; $i < $script_param{'wsplit'}; $i++) {
  push @{$dfix{'wordlist'}}, ($dar_files[$i][1]);
}

# Encode the data file index object to JSON and then write it to the
# index file with gzip compression
#
my $dfix_json = encode_json(\%dfix);
open(
    my $fhi, "> :gzip",
    File::Spec->catpath(
        $outdir_vol, $outdir_path, "cantotype_data_index.gz"))
  or die "Can't create 'cantotype_data_index.gz' for writing, stopped";

print { $fhi } "$dfix_json\n";
close($fhi);

# Generate all the character data parts and write to output directory
#
my $per_part = int(($#car + 1) / $script_param{'csplit'});
for(my $i = 0; $i < $script_param{'csplit'}; $i++) {
  
  # Compute the starting array index of this part
  my $start_i = $per_part * $i;
  
  # Usually, the number of records in a part is equal to $per_part, but
  # in the very last part, it is equal to all remaining records
  my $r_count = $per_part;
  if ($i >= $script_param{'csplit'} - 1) {
    $r_count = ($#car + 1) - $start_i;
  }
  ($r_count > 0) or die "Error splitting parts, stopped";
  
  # Grab the subset of records we are putting into this part
  my $last_i = $start_i + $r_count - 1;
  my @rss = @car[$start_i .. $last_i];

  # Encode subset into JSON
  my $jss = encode_json(\@rss);
  
  # Write the JSON to the data file, using gzip compression
  open(my $fhp, "> :gzip", $car_files[$i][0]) or
    die "Can't create '$car_files[$i][0]' for writing, stopped";
  
  print { $fhp } "$jss\n";
  close($fhp);
}

# Generate all the word data parts and write to output directory
#
$per_part = int(($#dar + 1) / $script_param{'wsplit'});
for(my $i = 0; $i < $script_param{'wsplit'}; $i++) {
  
  # Compute the starting array index of this part
  my $start_i = $per_part * $i;
  
  # Usually, the number of records in a part is equal to $per_part, but
  # in the very last part, it is equal to all remaining records
  my $r_count = $per_part;
  if ($i >= $script_param{'wsplit'} - 1) {
    $r_count = ($#dar + 1) - $start_i;
  }
  ($r_count > 0) or die "Error splitting parts, stopped";
  
  # Grab the subset of records we are putting into this part
  my $last_i = $start_i + $r_count - 1;
  my @rss = @dar[$start_i .. $last_i];

  # Encode subset into JSON
  my $jss = encode_json(\@rss);
  
  # Write the JSON to the data file, using gzip compression
  open(my $fhp, "> :gzip", $dar_files[$i][0]) or
    die "Can't create '$dar_files[$i][0]' for writing, stopped";
  
  print { $fhp } "$jss\n";
  close($fhp);
}

=head1 AUTHOR

Noah Johnson, C<noah.johnson@loupmail.com>

=head1 COPYRIGHT AND LICENSE

Copyright (C) 2022 Multimedia Data Technology Inc.

MIT License:

Permission is hereby granted, free of charge, to any person obtaining a
copy of this software and associated documentation files
(the "Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be included
in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

=cut
