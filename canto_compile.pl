#!/usr/bin/env perl
use strict;
use feature 'unicode_strings';
use warnings FATAL => "utf8";

# Non-core dependencies
#
use JSON::Tiny qw(decode_json encode_json);

# Core dependencies
#
use File::Spec;

=head1 NAME

canto_compile.pl - Compile a JavaScript table for Cantotype.

=head1 SYNOPSIS

  canto_compile.pl HKSCS.json Unihan/Directory cedict.txt > out.js

=head1 DESCRIPTION

This script reads data files from Hong Kong Supplemental Character Set
(HKSCS), Unihan, and CC-CEDICT and uses this data to compile JavaScript
dictionary data for Cantotype.

=head2 JavaScript result format

The JavaScript file generated by this script defines two global
variables, named C<canto_chars> and named C<canto_words>.  The generated
JavaScript file initializes these two global variables to (large!)
arrays containing sequences of data records.

For the C<canto_chars> array, each record is a JavaScript object whose
properties define the properties of a specific Chinese character.  The
following properties are always defined:

=over 4

=item C<cpv>

The numeric Unicode codepoint of the character.  Supplemental characters
are represented directly by their numeric codepoint rather than by a
surrogate pair.  Each record in the C<canto_chars> array has a unique
value for this field, but there is no guarantee of the ordering of
records in the table.  This field is an integer value.

=item C<crd>

The Cantonese reading(s) of the character.  The value is an array of one
or more strings, each of which contains a lowercase Jyutping reading.
No particular ordering is guaranteed within the readings.

=back

The following properties are optional, only defined if available:

=over 4

=item C<dfn>

A string providing a definition gloss of the character's meaning, in
English.  This is taken from the Unihan database, so it is not meant to
be an actual Chinese word definition, but rather just a gloss for that
specific character.

=back

For the C<canto_words> array, each record is itself an array.  The
elements of the array are as follows:

=over 4

=item Element 0

String value holding the traditional character(s) for the word.  For
western names, double-width middle dots may be present.  For proverbs,
double-width commas may be present.

=item Element 1

String value holding the simplified character(s) for the word.  For
western names, double-width middle dots may be present.  For proverbs,
double-width commas may be present.

=item Element 2

Array of strings indicating the Mandarin Pinyin reading of the word.  If
the array has multiple elements, this means a multi-syllable
pronunication.  It does not mean each element is an alternative.  Pinyin
syllables may have their initial letters capitalized for proper names.
Diacritic marks are not used.  Instead, tone is represented by an
integer value 1-5 suffixed to the syllable, with 5 representing neutral
tone.  Also, the U-umlaut character is represented by the letter U
followed by a colon.  Finally, if a double-width middle dot or a
double-width comma was part of the prior record elements, those
punctuation marks will appear as their own element within this reading
array, with a regular middle dot and regular comma used instead of the
double-width varieties.

=item Element 3

Array of strings, each containing a separate English definition of the
Chinese word.  Chinese characters might be included within these
definitions, so it is not safe to assume they are ASCII.

=back

=head2 Character table range

The character table contains a subset of the Chinese characters in
Unicode.  Specifically, the table only includes characters that have a
Big5 mapping defined in the Unihan database B<or> appear in the HKSCS
supplement of Cantonese-specific characters.  Furthermore, only
characters that have at least one Cantonese reading will be included.

=head2 Jyutping format

The C<VALID_INITIALS> and C<VALID_FINALS> variables give the recognized
valid initials and finals in Jyutping syllables.  The tone numbers may
be 1-6.  Also, finals "m" and "ng" may stand by themselves without a
vowel in the final.

=head2 Data sources

The HKSCS file is a JSON file that has a JSON array at the top level.
Each element in this JSON array is a JSON object.  These object elements
must at least have a C<codepoint> property and a C<cantonese> property.
The C<codepoint> property has a string value that contains the Unicode
codepoint as base-16 digits.  The C<cantonese> property is also a
string.  If it is empty, the record is ignored.  Otherwise, it must
consist of one or more Jyutping romanizations, separated by commas.

The HKSCS file is B<not> a complete listing of Cantonese characters.
Instead, it only contains special Cantonese characters that are beyond
the baseline Big5 standard.  You can get a file C<HKSCS2016.json> that
is entitled I<Hong Kong Supplementary Character Set related information
(JSON format)> which has the necessary format from the Office of the
Government Chief Information Officer of Hong Kong, Common Chinese
Language Interface, Download Area, at:

C<https://www.ogcio.gov.hk/en/our_work/business/tech_promotion/ccli/download_area/>

The Unihan files are available from the Unicode consortium in an archive
called C<unihan.zip>.  The archive is available at:

C<https://www.unicode.org/Public/UCD/latest/ucd/Unihan.zip>

This script requires the path to a directory that contains all of the
text files extracted from C<unihan.zip>.

The HKSCS and Unihan data files have some typos and obscure characters
in them that this script automatically corrects.  The correction
procedure is smart enough that if the datasets are fixed in the futures,
the corrections will no longer be applied in the future if no longer
necessary.  See C<correct_cmap> function for details of the corrections
that are applied.

Finally, the script requires the CC-CEDICT Chinese dictionary data file.
This file is available for download from C<www.mdbg.net>.  You must
decompress it before passing it to this script.

=cut

# =========
# Constants
# =========

# All of the valid Jyutping initials, in lowercase, with each initial
# surrounded by colons
#
my $VALID_INITIALS = ":b:p:m:f:d:t:n:l:g:k:ng:h:gw:kw:w:z:c:s:j:";

# All of the valid Jyutping finals, in lowercase, with each final
# surrounded by colons
#
# NOTE: "et" final added in here because it is used in multiple entries
#
my $VALID_FINALS =
  ":aa:aai:aau:aam:aan:aang:aap:aat:aak" .
  ":a:ai:au:am:an:ang:ap:at:ak" .
  ":e:ei:eu:em:eng:ep:et:ek" .
  ":i:iu:im:in:ing:ip:it:ik" .
  ":o:oi:ou:on:ong:ot:ok" .
  ":u:ui:un:ung:ut:uk" .
  ":eoi:eon:eot" .
  ":oe:oeng:oet:oek" .
  ":yu:yun:yut" .
  ":m:ng:";

# ===============
# Local functions
# ===============

# Given a reference to the %cmap hash, apply corrections for typos and
# obscure characters in the input datasets.
#
# The following obscure codepoints with non-conformant Jyutping are
# DROPPED from the %cmap if present:
#
#   U+297C4 ("qi1")
#   U+2BAC8 ("bop6")
#
# The following codepoints, if present, have the following corrections
# made to their readings, if the error version is present:
#
#   U+5414  yaa1   -> jaa1
#   U+667B  om2    -> am2
#   U+27C3C zeong6 -> zoeng6
#   U+2BAF2 zoen6  -> zeon6
#   U+2BB1B zoen6  -> zeon6
#   U+2F817 yung2  -> jung2
#
# Parameters:
#
#   1 : hash ref - the %cmap to apply corrections to
#
sub correct_cmap {
  # Check number of parameters
  ($#_ == 0) or die "Wrong number of parameters, stopped";
  
  # Get argument and check type
  my $cm = shift;
  (ref($cm) eq "HASH") or die "Wrong argument type, stopped";
  
  # Drop the obscure codepoints if present
  for my $cpv (0x297c4, 0x2bac8) {
    if (exists $cm->{"$cpv"}) {
      delete $cm->{"$cpv"};
    }
  }
  
  # Apply corrections
  for my $ca ([ 0x5414, "yaa1"  , "jaa1"  ],
              [ 0x667b, "om2"   , "am2"   ],
              [0x27c3c, "zeong6", "zoeng6"],
              [0x2baf2, "zoen6" , "zeon6" ],
              [0x2bb1b, "zoen6" , "zeon6" ],
              [0x2f817, "yung2" , "jung2" ]) {
    
    # Get parameters for this correction
    my $cpv = $ca->[0];
    my $cer = $ca->[1];
    my $crp = $ca->[2];

    # Skip if codepoint not present
    if (not exists $cm->{"$cpv"}) {
      next;
    }

    # Get reference to readings array
    my $ra = $cm->{"$cpv"};
    
    # Starting from last element of readings array and going to first,
    # drop any readings that are erroneous and set the found_err flag if
    # at least one reading dropped
    my $found_err = 0;
    for(my $i = (scalar @$ra - 1); $i >= 0; $i--) {
      if ($ra->[$i] eq $cer) {
        $found_err = 1;
        splice @$ra, $i, 1;
      }
    }

    # Skip rest of processing if no error reading found
    if (not $found_err) {
      next;
    }
    
    # Check whether the correction is already in the readings array
    my $already = 0;
    for my $r (@$ra) {
      if ($r eq $crp) {
        $already = 1;
        last;
      }
    }

    # Add correction if not already in readings array
    if (not $already) {
      push @$ra, ($crp);
    }
  }
}

# Check whether a given string is a valid Jyutping syllable.
#
# Parameters:
#
#   1 : string - the string to check
#
# Return:
#
#   1 if string is valid Jyutping syllable, 0 if not
#
sub check_reading {
  # Check number of parameters
  ($#_ == 0) or die "Wrong number of parameters, stopped";
  
  # Get argument as string
  my $str = shift;
  $str = "$str";
  
  # Check for tone suffix and get the main syllable
  ($str =~ /^([a-z]+)[1-6]$/u) or return 0;
  $str = $1;
  
  # Handle syllabic m and ng as special case
  if ($str =~ /^([^aeiouy]*)(?:m|ng)$/u) {
    # Get the initial
    $str = $1;
    
    # If initial is not empty, check that initial is valid
    ((length($str) < 1) or ($VALID_INITIALS =~ /:$str:/u)) or return 0;
    
    # If we got here, syllable is valid as special case with syllabic
    # m or ng
    return 1;
  }
  
  # Split into initial (which may be empty) and final (required)
  ($str =~ /^([^aeiouy]*)([aeiouy].*)$/u) or return 0;
  my $i = $1;
  my $f = $2;
  
  # If initial is not empty, check that initial is valid
  ((length($i) < 1) or ($VALID_INITIALS =~ /:$i:/u)) or return 0;
  
  # Check that final is valid
  ($VALID_FINALS =~ /:$f:/u) or return 0;
  
  # If we got here, syllable is valid Jyutping
  return 1;
}

# Given a path to a directory and a filename within that directory,
# return the full path to that file within the directory.
#
# Parameters:
#
#   1 : string - the path to the directory
#
#   2 : string - the filename
#
# Return:
#
#   string - the path to the file
#
sub subfile {
  # Check parameter count
  ($#_ == 1) or die "Wrong number of parameters, stopped";
  
  # Get parameters as strings
  my $path_dir  = shift;
  my $path_name = shift;
  
  $path_dir  = "$path_dir";
  $path_name = "$path_name";
  
  # Split the directory
  (my $dvol, my $ddir, undef) = File::Spec->splitpath($path_dir, 1);
  
  # Return the full path
  return File::Spec->catpath($dvol, $ddir, $path_name);
}

# ==================
# Program entrypoint
# ==================

# Check that we got three parameters
#
($#ARGV == 2) or die "Wrong number of parameters, stopped";

# Store the parameters
#
my $path_hkscs  = $ARGV[0];
my $path_unihan = $ARGV[1];
my $path_dict   = $ARGV[2];

# Check that paths exist
#
(-f $path_hkscs) or die "Can't find file '$path_hkscs', stopped";
(-d $path_unihan) or die "Can't find directory '$path_unihan', stopped";
(-f $path_dict) or die "Can't find file '$path_dict', stopped";

# Define Unihan data file paths
#
my $unihan_irg   = subfile($path_unihan, "Unihan_IRGSources.txt");
my $unihan_other = subfile($path_unihan, "Unihan_OtherMappings.txt");
my $unihan_radst = subfile($path_unihan,
                              "Unihan_RadicalStrokeCounts.txt");
my $unihan_read  = subfile($path_unihan, "Unihan_Readings.txt");

# Check for existence of Unihan data files
#
(-f $unihan_irg) or die "Can't find file '$unihan_irg', stopped";
(-f $unihan_other) or die "Can't find file '$unihan_other', stopped";
(-f $unihan_radst) or die "Can't find file '$unihan_radst', stopped";
(-f $unihan_read) or die "Can't file file '$unihan_read', stopped";

# Start with an empty hash, which will map decimal integer codepoints to
# array references containing Jyutping romanizations
#
my %cmap;

# First, open the other mappings file
#
open(my $fhm, "< :utf8", $unihan_other) or
  die "Failed to open '$unihan_other', stopped";

# Process mappings file line by line and add all Big5 Unicode codepoints
# to the hash, with empty array reference values for now
#
while (<$fhm>) {
  # Skip line if blank
  if (/^[ \t\r\n]*$/u) {
    next;
  }
  
  # Skip line if first character is # indicating comment
  if (/^[ \t]*#/u) {
    next;
  }
  
  # If this is a Big5 record, add to the hash
  if (/^[ \t]*U\+([0-9a-fA-F]{4,6})\tkBigFive\t/u) {
    my $cpv = hex($1);
    $cmap{"$cpv"} = [];
  }
}

# Close the mappings file
#
close($fhm);

# Second, open the readings file
#
open(my $fhr, "< :utf8", $unihan_read) or
  die "Failed to open '$unihan_read', stopped";

# Process readings file line by line, and for any Cantonese reading
# where the codepoint is already in the mappings file (indicating that
# it is in the core Big5 set), push all readings onto the array value,
# after making sure the array doesn't already contain that reading
#
while (<$fhr>) {
  # Skip line if blank
  if (/^[ \t\r\n]*$/u) {
    next;
  }
  
  # Skip line if first character is # indicating comment
  if (/^[ \t]*#/u) {
    next;
  }
  
  # If this is a Cantonese reading record, process it
  if (/^[ \t]*U\+([0-9a-fA-F]{4,6})\tkCantonese\t(.+)[\r\n]*$/u) {
    my $cpv = hex($1);
    my $rstr = $2;
    
    # Check that exactly one syllable defined
    ($rstr =~ /^[A-Za-z]+[1-6]$/u) or
      die "Invalid kCantonese value: $rstr stopped";
    
    # Check if reading already present
    my $already = 0;
    for my $r (@{$cmap{"$cpv"}}) {
      if ($r eq $rstr) {
        $already = 1;
        last;
      }
    }
    
    # If not already defined, add the reading
    if (not $already) {
      push @{$cmap{"$cpv"}}, ($rstr);
    }
  }
}

# Close the readings file
#
close($fhr);

# Open the HKSCS file in raw mode
#
open(my $fhh, "< :raw", $path_hkscs) or
  die "Failed to open '$path_hkscs', stopped";

# Slurp the whole HKSCS file into memory
#
my $hkscs;
{
  local $/;
  $hkscs = <$fhh>;
}

# Close HKSCS file
#
close($fhh);

# If file starts with UTF-8 BOM, remove it
#
if (length $hkscs > 3) {
  if ((ord(substr($hkscs, 0, 1)) == 0xef) and
      (ord(substr($hkscs, 1, 1)) == 0xbb) and
      (ord(substr($hkscs, 2, 1)) == 0xbf)) {
    $hkscs = substr($hkscs, 3);
  }
}

# Decode JSON
#
$hkscs = decode_json($hkscs);

# Make sure top-level JSON is array
#
(ref($hkscs) eq "ARRAY") or
  die "HKSCS must be JSON array, stopped";

# Go through all JSON records, and for each that has a Cantonese
# reading, add it to the hash, making sure it's not already in there
#
for my $h (@$hkscs) {
  
  # Make sure element is hash reference
  (ref($h) eq "HASH") or
    die "HKSCS elements must be JSON objects, stopped";
  
  # Make sure required codepoint and cantonese properties are there
  (exists $h->{'codepoint'}) or
    die "HKSCS elements must all have codepoint properties, stopped";
  (exists $h->{'cantonese'}) or
    die "HKSCS elements must all have cantonese properties, stopped";
  
  # Get the properties
  my $cpv  = $h->{'codepoint'};
  my $cstr = $h->{'cantonese'};
  
  # Skip if cantonese property is empty
  if ($cstr =~ /^[ \t]*$/u) {
    next;
  }
  
  # Parse the codepoint
  $cpv = hex($cpv);
  
  # Replace periods with commas -- this will fix the "nuk6." typo in
  # HKSCS2016
  $cstr =~ s/\./,/ug;
  
  # Replace all commas with comma followed by space so that there is a
  # space everywhere after commas -- this will fix the "taan3,wan6" typo
  # that is missing a space in HKSCS2016
  $cstr =~ s/,/, /ug;
  
  # Drop all commas -- we can parse the syllables with spaces, and there
  # is a typo in HKSCS2016 at "jing1 mau5" with a missing comma that
  # will be fixed if we go by whitespace instead
  $cstr =~ s/,//ug;
  
  # Trim leading and trailing whitespace
  $cstr =~ s/^[ \t]+//gu;
  $cstr =~ s/[ \t]+$//gu;
  
  # Split by whitespace
  my @car = split " ", $cstr;
  
  # Process each reading
  for my $r (@car) {
    # If codepoint doesn't exist yet, add it with empty array
    if (not exists $cmap{"$cpv"}) {
      $cmap{"$cpv"} = [];
    }
    
    # Check whether reading already present in array
    my $already = 0;
    for my $s (@{$cmap{"$cpv"}}) {
      if ($s eq $r) {
        $already = 1;
        last;
      }
    }
    
    # If not already defined, add the reading
    if (not $already) {
      push @{$cmap{"$cpv"}}, ($r);
    }
  }
}

# Apply corrections to %cmap
#
correct_cmap(\%cmap);

# We now have a mapping of Unicode codepoints to all their readings;
# open the readings file again so we can make another pass and this time
# build the definitions table for any codepoints that are in our %cmap
#
open(my $fhd, "< :utf8", $unihan_read) or
  die "Failed to open '$unihan_read', stopped";

# Process readings file line by line, and build %dmap that maps
# lowercase base-16 codepoint values to definition strings, but only for
# codepoints that are in %cmap
#
my %dmap;
while (<$fhd>) {
  # Skip line if blank
  if (/^[ \t\r\n]*$/u) {
    next;
  }
  
  # Skip line if first character is # indicating comment
  if (/^[ \t]*#/u) {
    next;
  }
  
  # If this is a definition record, process it
  if (/^[ \t]*U\+([0-9a-fA-F]{4,6})\tkDefinition\t(.+)[\r\n]*$/u) {
    my $cpv = hex($1);
    my $dstr = $2;
    
    # Skip this definition record if it is not in the %cmap
    if (not exists $cmap{$cpv}) {
      next;
    }
    
    # Get the lowercase hex version of the codepoint value
    $cpv = sprintf("%x", $cpv);
    
    # Make sure codepoint not already defined
    (not exists $dmap{$cpv}) or
      die "Codepoint $cpv has multiple kDefinitions, stopped";
    
    # Store the definition
    $dmap{$cpv} = $dstr;
  }
}

# Close the readings file
#
close($fhd);

# Build a mapping of readings to their Unicode codepoints, and check
# along the way that all readings follow Jyutping romanization
#
my %rmap;
for my $cpv (keys %cmap) {
  # Get the integer value of codepoint
  my $cpi = int($cpv);
  
  # Process each reading
  for my $r (@{$cmap{"$cpv"}}) {
    # Verify reading
    (check_reading($r)) or
      die "Invalid Jyutping '$r', stopped";
    
    # If reading not yet defined, add it to rmap
    if (not exists $rmap{$r}) {
      $rmap{$r} = [];
    }
    
    # Push the integer codepoint value onto reading array
    push @{$rmap{$r}}, ($cpi);
  }
}

# Go through the generated map and for each key, sort the array it maps
# to so codepoints are in ascending order
for my $k (keys %rmap) {
  my @sa = sort { $a <=> $b } @{$rmap{$k}};
  $rmap{$k} = \@sa;
}

# Encode the mapping of readings to codepoints into JSON
#
my $rmap_json = encode_json(\%rmap);

# Encode the definition table into JSON
#
my $dmap_json = encode_json(\%dmap);

# Print the completed JavaScript to output
#
print "var canto_table = $rmap_json;\n";
print "var canto_define = $dmap_json;\n";

=head1 AUTHOR

Noah Johnson, C<noah.johnson@loupmail.com>

=head1 COPYRIGHT AND LICENSE

Copyright (C) 2022 Multimedia Data Technology Inc.

MIT License:

Permission is hereby granted, free of charge, to any person obtaining a
copy of this software and associated documentation files
(the "Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be included
in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

=cut
